{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another popular text analysis technique is called topic modeling. The ultimate goal of topic modeling is to find various topics that are present in your corpus. Each document in the corpus will be made up of at least one topic, if not multiple topics.\n",
    "\n",
    "In this notebook, we will be covering the steps on how to do **Latent Dirichlet Allocation (LDA)**, which is one of many topic modeling techniques. It was specifically designed for text data.\n",
    "\n",
    "To use a topic modeling technique, you need to provide (1) a document-term matrix and (2) the number of topics you would like the algorithm to pick up.\n",
    "\n",
    "Once the topic modeling technique is applied, your job as a human is to interpret the results and see if the mix of words in each topic make sense. If they don't make sense, you can try changing up the number of topics, the terms in the document-term matrix, model parameters, or even try a different model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sailo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\sailo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\sailo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling - Attempt #1 (All Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaaaah</th>\n",
       "      <th>aaah</th>\n",
       "      <th>aah</th>\n",
       "      <th>abc</th>\n",
       "      <th>abcs</th>\n",
       "      <th>ability</th>\n",
       "      <th>abject</th>\n",
       "      <th>able</th>\n",
       "      <th>abortion</th>\n",
       "      <th>abortions</th>\n",
       "      <th>...</th>\n",
       "      <th>zeppelin</th>\n",
       "      <th>zero</th>\n",
       "      <th>zillion</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zombies</th>\n",
       "      <th>zone</th>\n",
       "      <th>zones</th>\n",
       "      <th>zoning</th>\n",
       "      <th>zoo</th>\n",
       "      <th>zoom</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ali</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anthony</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bill</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dave</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gabriel</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hasan</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jim</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joe</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>john</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kevin</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>louis</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mike</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ricky</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13 rows × 7485 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         aaaaah  aaah  aah  abc  abcs  ability  abject  able  abortion  \\\n",
       "ali           0     0    0    1     0        0       0     2         0   \n",
       "anthony       0     0    0    0     0        0       0     0         2   \n",
       "bill          1     0    0    0     1        0       0     1         0   \n",
       "dave          0     1    0    0     0        0       0     0         0   \n",
       "gabriel       0     1    0    1     0        0       0     0         0   \n",
       "hasan         0     0    0    0     0        0       0     1         0   \n",
       "jim           0     0    0    0     0        0       0     3         0   \n",
       "joe           0     0    0    0     0        0       0     2         0   \n",
       "john          0     0    0    0     0        0       0     3         0   \n",
       "kevin         0     0    3    0     0        0       0     2         0   \n",
       "louis         0     0    3    0     0        0       0     1         0   \n",
       "mike          0     0    0    0     0        0       0     0         0   \n",
       "ricky         0     0    0    0     0        1       1     2         0   \n",
       "\n",
       "         abortions  ...  zeppelin  zero  zillion  zombie  zombies  zone  \\\n",
       "ali              0  ...         0     0        0       1        0     0   \n",
       "anthony          0  ...         0     0        0       0        0     0   \n",
       "bill             0  ...         0     1        1       1        1     0   \n",
       "dave             1  ...         0     0        0       0        0     0   \n",
       "gabriel          0  ...         0     0        0       0        0     0   \n",
       "hasan            0  ...         0     1        0       0        0     0   \n",
       "jim              0  ...         0     0        0       0        0     0   \n",
       "joe              0  ...         0     0        0       0        0     0   \n",
       "john             0  ...         0     0        0       0        0     0   \n",
       "kevin            0  ...         0     0        0       0        1     1   \n",
       "louis            0  ...         0     2        0       0        0     0   \n",
       "mike             0  ...         2     1        0       0        0     0   \n",
       "ricky            0  ...         0     0        0       0        0     0   \n",
       "\n",
       "         zones  zoning  zoo  zoom  \n",
       "ali          0       0    0     0  \n",
       "anthony      0       0    0     0  \n",
       "bill         0       1    0     0  \n",
       "dave         0       0    0     0  \n",
       "gabriel      1       0    0     4  \n",
       "hasan        0       0    0     0  \n",
       "jim          0       0    0     0  \n",
       "joe          0       0    0     0  \n",
       "john         0       0    0     0  \n",
       "kevin        0       0    0     0  \n",
       "louis        0       0    0     0  \n",
       "mike         0       0    0     0  \n",
       "ricky        0       0    1     0  \n",
       "\n",
       "[13 rows x 7485 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's read in our document-term matrix\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "data = pd.read_pickle('dtm_stop.pkl')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the necessary modules for LDA with gensim\n",
    "# Terminal / Anaconda Navigator: conda install -c conda-forge gensim\n",
    "from gensim import matutils, models\n",
    "import scipy.sparse\n",
    "\n",
    "# import logging\n",
    "# logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ali</th>\n",
       "      <th>anthony</th>\n",
       "      <th>bill</th>\n",
       "      <th>dave</th>\n",
       "      <th>gabriel</th>\n",
       "      <th>hasan</th>\n",
       "      <th>jim</th>\n",
       "      <th>joe</th>\n",
       "      <th>john</th>\n",
       "      <th>kevin</th>\n",
       "      <th>louis</th>\n",
       "      <th>mike</th>\n",
       "      <th>ricky</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>aaaaah</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aaah</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aah</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abc</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abcs</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ali  anthony  bill  dave  gabriel  hasan  jim  joe  john  kevin  \\\n",
       "aaaaah    0        0     1     0        0      0    0    0     0      0   \n",
       "aaah      0        0     0     1        1      0    0    0     0      0   \n",
       "aah       0        0     0     0        0      0    0    0     0      3   \n",
       "abc       1        0     0     0        1      0    0    0     0      0   \n",
       "abcs      0        0     1     0        0      0    0    0     0      0   \n",
       "\n",
       "        louis  mike  ricky  \n",
       "aaaaah      0     0      0  \n",
       "aaah        0     0      0  \n",
       "aah         3     0      0  \n",
       "abc         0     0      0  \n",
       "abcs        0     0      0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One of the required inputs is a term-document matrix\n",
    "tdm = data.transpose()\n",
    "tdm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We're going to put the term-document matrix into a new gensim format, from df --> sparse matrix --> gensim corpus\n",
    "sparse_counts = scipy.sparse.csr_matrix(tdm)\n",
    "corpus = matutils.Sparse2Corpus(sparse_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\anaconda\\lib\\site-packages\\sklearn\\base.py:299: UserWarning: Trying to unpickle estimator CountVectorizer from version 1.0.2 when using version 1.2.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Gensim also requires dictionary of the all terms and their respective location in the term-document matrix\n",
    "cv = pickle.load(open(\"cv_stop.pkl\", \"rb\"))\n",
    "id2word = dict((v, k) for k, v in cv.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the corpus (term-document matrix) and id2word (dictionary of location: term), we need to specify two other parameters - the number of topics and the number of passes. Let's start the number of topics at 2, see if the results make sense, and increase the number from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.027*\"like\" + 0.019*\"just\" + 0.016*\"don\" + 0.014*\"know\" + 0.010*\"right\" + 0.010*\"gonna\" + 0.009*\"people\" + 0.009*\"shit\" + 0.009*\"got\" + 0.007*\"cking\"'),\n",
       " (1,\n",
       "  '0.030*\"like\" + 0.017*\"know\" + 0.013*\"don\" + 0.012*\"just\" + 0.011*\"said\" + 0.010*\"right\" + 0.007*\"got\" + 0.007*\"people\" + 0.006*\"going\" + 0.005*\"say\"')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now that we have the corpus (term-document matrix) and id2word (dictionary of location: term),\n",
    "# we need to specify two other parameters as well - the number of topics and the number of passes\n",
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=2, passes=10)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.024*\"like\" + 0.016*\"don\" + 0.014*\"know\" + 0.013*\"right\" + 0.013*\"just\" + 0.012*\"said\" + 0.008*\"got\" + 0.007*\"went\" + 0.006*\"people\" + 0.006*\"fucking\"'),\n",
       " (1,\n",
       "  '0.027*\"like\" + 0.015*\"just\" + 0.014*\"don\" + 0.012*\"people\" + 0.012*\"know\" + 0.008*\"gonna\" + 0.008*\"shit\" + 0.007*\"cking\" + 0.006*\"got\" + 0.005*\"think\"'),\n",
       " (2,\n",
       "  '0.033*\"like\" + 0.018*\"know\" + 0.016*\"just\" + 0.013*\"don\" + 0.010*\"right\" + 0.008*\"said\" + 0.008*\"got\" + 0.007*\"people\" + 0.007*\"gonna\" + 0.006*\"shit\"')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LDA for num_topics = 3\n",
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=3, passes=10)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.034*\"like\" + 0.017*\"know\" + 0.016*\"just\" + 0.014*\"don\" + 0.012*\"right\" + 0.009*\"said\" + 0.009*\"got\" + 0.007*\"yeah\" + 0.007*\"people\" + 0.006*\"gonna\"'),\n",
       " (1,\n",
       "  '0.028*\"like\" + 0.017*\"don\" + 0.016*\"just\" + 0.014*\"know\" + 0.011*\"people\" + 0.010*\"right\" + 0.008*\"said\" + 0.008*\"got\" + 0.007*\"gonna\" + 0.007*\"cking\"'),\n",
       " (2,\n",
       "  '0.021*\"like\" + 0.019*\"know\" + 0.010*\"said\" + 0.009*\"just\" + 0.008*\"don\" + 0.007*\"right\" + 0.007*\"going\" + 0.006*\"got\" + 0.006*\"okay\" + 0.006*\"yeah\"'),\n",
       " (3,\n",
       "  '0.001*\"like\" + 0.001*\"know\" + 0.000*\"just\" + 0.000*\"don\" + 0.000*\"people\" + 0.000*\"right\" + 0.000*\"got\" + 0.000*\"gonna\" + 0.000*\"shit\" + 0.000*\"said\"')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LDA for num_topics = 4\n",
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=4, passes=10)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These topics aren't looking too great. We've tried modifying our parameters. Let's try modifying our terms list as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling - Attempt #2 (Nouns Only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One popular trick is to look only at terms that are from one part of speech (only nouns, only adjectives, etc.). Check out the UPenn tag set: https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's create a function to pull out nouns from a string of text\n",
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "def nouns(text):\n",
    "    '''Given a string of text, tokenize the text and pull out only the nouns.'''\n",
    "    is_noun = lambda pos: pos[:2] == 'NN'\n",
    "    tokenized = word_tokenize(text)\n",
    "    all_nouns = [word for (word, pos) in pos_tag(tokenized) if is_noun(pos)] \n",
    "    return ' '.join(all_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "      <th>full_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ali</th>\n",
       "      <td>ladies and gentlemen please welcome the stage ...</td>\n",
       "      <td>Ali Wong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anthony</th>\n",
       "      <td>thank you thank you thank you san francisco th...</td>\n",
       "      <td>Anthony Jeselnik</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bill</th>\n",
       "      <td>all right thank you thank you very much thank ...</td>\n",
       "      <td>Bill Burr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dave</th>\n",
       "      <td>this dave tells dirty jokes for living that st...</td>\n",
       "      <td>Dave Chappelle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gabriel</th>\n",
       "      <td>can you please state your name martin moreno b...</td>\n",
       "      <td>Gabriel Iglesias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hasan</th>\n",
       "      <td>what davis what home had bring back here netfl...</td>\n",
       "      <td>Hasan Minhaj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jim</th>\n",
       "      <td>ladies and gentlemen please welcome the stage ...</td>\n",
       "      <td>Jim Jefferies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joe</th>\n",
       "      <td>ladies and gentlemen welcome joe rogan what th...</td>\n",
       "      <td>Joe Rogan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>john</th>\n",
       "      <td>armed with boyish charm and sharp wit the form...</td>\n",
       "      <td>John Mulaney</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kevin</th>\n",
       "      <td>streaming netflix from november what was looki...</td>\n",
       "      <td>Kevin Hart</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>louis</th>\n",
       "      <td>intro fade the music out let roll hold there l...</td>\n",
       "      <td>Louis C.K.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mike</th>\n",
       "      <td>wow hey thank you thanks thank you guys hey se...</td>\n",
       "      <td>Mike Birbiglia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ricky</th>\n",
       "      <td>hello hello how you doing great thank you wow ...</td>\n",
       "      <td>Ricky Gervais</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                transcript         full_name\n",
       "ali      ladies and gentlemen please welcome the stage ...          Ali Wong\n",
       "anthony  thank you thank you thank you san francisco th...  Anthony Jeselnik\n",
       "bill     all right thank you thank you very much thank ...         Bill Burr\n",
       "dave     this dave tells dirty jokes for living that st...    Dave Chappelle\n",
       "gabriel  can you please state your name martin moreno b...  Gabriel Iglesias\n",
       "hasan    what davis what home had bring back here netfl...      Hasan Minhaj\n",
       "jim      ladies and gentlemen please welcome the stage ...     Jim Jefferies\n",
       "joe      ladies and gentlemen welcome joe rogan what th...         Joe Rogan\n",
       "john     armed with boyish charm and sharp wit the form...      John Mulaney\n",
       "kevin    streaming netflix from november what was looki...        Kevin Hart\n",
       "louis    intro fade the music out let roll hold there l...        Louis C.K.\n",
       "mike     wow hey thank you thanks thank you guys hey se...    Mike Birbiglia\n",
       "ricky    hello hello how you doing great thank you wow ...     Ricky Gervais"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the cleaned data, before the CountVectorizer step\n",
    "data_clean = pd.read_pickle('data_clean.pkl')\n",
    "data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ali</th>\n",
       "      <td>ladies gentlemen stage ali thank hello na shit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anthony</th>\n",
       "      <td>thank thank people told tape francisco city wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bill</th>\n",
       "      <td>thank thank georgia area oasis kind heat racis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dave</th>\n",
       "      <td>dave jokes living work signifies train alchemi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gabriel</th>\n",
       "      <td>state name martin moreno gabriel iglesias year...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hasan</th>\n",
       "      <td>home netflix york son davis california year cl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jim</th>\n",
       "      <td>ladies gentlemen stage jim jefferies thank end...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joe</th>\n",
       "      <td>ladies gentlemen joe thanks god damn phone ckf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>john</th>\n",
       "      <td>charm wit snl writer john mulaney marriage bee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kevin</th>\n",
       "      <td>netflix house downstairs work homework eyes ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>louis</th>\n",
       "      <td>intro music let roll lights thank appreciate p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mike</th>\n",
       "      <td>wow hey thanks see look insane years everyone ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ricky</th>\n",
       "      <td>hello thank fuck thank gon tonight money worth...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                transcript\n",
       "ali      ladies gentlemen stage ali thank hello na shit...\n",
       "anthony  thank thank people told tape francisco city wo...\n",
       "bill     thank thank georgia area oasis kind heat racis...\n",
       "dave     dave jokes living work signifies train alchemi...\n",
       "gabriel  state name martin moreno gabriel iglesias year...\n",
       "hasan    home netflix york son davis california year cl...\n",
       "jim      ladies gentlemen stage jim jefferies thank end...\n",
       "joe      ladies gentlemen joe thanks god damn phone ckf...\n",
       "john     charm wit snl writer john mulaney marriage bee...\n",
       "kevin    netflix house downstairs work homework eyes ho...\n",
       "louis    intro music let roll lights thank appreciate p...\n",
       "mike     wow hey thanks see look insane years everyone ...\n",
       "ricky    hello thank fuck thank gon tonight money worth..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the nouns function to the transcripts to filter only on nouns\n",
    "data_nouns = pd.DataFrame(data_clean.transcript.apply(nouns))\n",
    "data_nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ladies</th>\n",
       "      <th>gentlemen</th>\n",
       "      <th>stage</th>\n",
       "      <th>ali</th>\n",
       "      <th>thank</th>\n",
       "      <th>hello</th>\n",
       "      <th>na</th>\n",
       "      <th>shit</th>\n",
       "      <th>cause</th>\n",
       "      <th>minutes</th>\n",
       "      <th>...</th>\n",
       "      <th>nieces</th>\n",
       "      <th>tissues</th>\n",
       "      <th>tissue</th>\n",
       "      <th>madhouse</th>\n",
       "      <th>beeline</th>\n",
       "      <th>adversity</th>\n",
       "      <th>boot</th>\n",
       "      <th>packet</th>\n",
       "      <th>copper</th>\n",
       "      <th>helmets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ali</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anthony</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bill</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dave</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gabriel</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hasan</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jim</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joe</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>john</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kevin</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>louis</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mike</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ricky</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13 rows × 4614 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ladies  gentlemen  stage  ali  thank  hello  na  shit  cause  \\\n",
       "ali           0          0      0    1      0      0   0     0      0   \n",
       "anthony       0          0      0    0      0      0   2     0      0   \n",
       "bill          1          0      0    0      1      0   0     0      0   \n",
       "dave          0          0      0    0      0      0   0     1      0   \n",
       "gabriel       0          1      0    1      0      0   0     0      0   \n",
       "hasan         0          0      0    0      0      0   0     0      0   \n",
       "jim           0          0      0    0      0      0   0     0      0   \n",
       "joe           0          0      0    0      0      0   0     0      1   \n",
       "john          0          0      0    0      0      0   0     0      0   \n",
       "kevin         0          0      2    0      0      0   0     0      0   \n",
       "louis         0          0      1    0      0      0   0     0      0   \n",
       "mike          0          0      0    0      0      0   0     0      0   \n",
       "ricky         0          0      0    0      0      1   0     0      0   \n",
       "\n",
       "         minutes  ...  nieces  tissues  tissue  madhouse  beeline  adversity  \\\n",
       "ali            0  ...       0        0       0         0        0          0   \n",
       "anthony        0  ...       0       10       0         0        0          0   \n",
       "bill           0  ...       0        0       0         0        1          1   \n",
       "dave           0  ...       0        0       0         0        0          0   \n",
       "gabriel        6  ...       0        0       0         0        0          0   \n",
       "hasan          0  ...       0        0       2         0        0          0   \n",
       "jim            0  ...       0        0       0         0        0          0   \n",
       "joe            0  ...       0        0       0         0        0          0   \n",
       "john           0  ...       0        0       0         0        0          0   \n",
       "kevin          0  ...       0        0       0         0        0          0   \n",
       "louis          0  ...       0        0       0         0        0          0   \n",
       "mike           0  ...       0        0       0         2        0          0   \n",
       "ricky          0  ...       1        0       0         0        0          0   \n",
       "\n",
       "         boot  packet  copper  helmets  \n",
       "ali         0       0       0        0  \n",
       "anthony     0       0       0        0  \n",
       "bill        1       0       0        0  \n",
       "dave        0       0       0        0  \n",
       "gabriel     0       1       0        4  \n",
       "hasan       0       0       0        0  \n",
       "jim         0       0       0        0  \n",
       "joe         0       0       0        0  \n",
       "john        0       0       0        0  \n",
       "kevin       1       0       0        0  \n",
       "louis       0       0       0        0  \n",
       "mike        0       0       0        0  \n",
       "ricky       0       0       1        0  \n",
       "\n",
       "[13 rows x 4614 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new document-term matrix using only nouns\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Re-add the additional stop words since we are recreating the document-term matrix\n",
    "add_stop_words = ['like', 'im', 'know', 'just', 'dont', 'thats', 'right', 'people','youre', 'got', 'gonna', 'time', 'think', 'yeah', 'said']\n",
    "a_stop_words = list(text.ENGLISH_STOP_WORDS.union(add_stop_words))\n",
    "\n",
    "# Recreate a document-term matrix with only nouns\n",
    "cvn = CountVectorizer(stop_words=a_stop_words)\n",
    "data_cvn = cvn.fit_transform(data_nouns.transcript)\n",
    "data_dtmn = pd.DataFrame(data_cvn.toarray(), columns=cvn.vocabulary_)\n",
    "data_dtmn.index = data_nouns.index\n",
    "data_dtmn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the gensim corpus\n",
    "corpusn = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(data_dtmn.transpose()))\n",
    "\n",
    "# Create the vocabulary dictionary\n",
    "id2wordn = dict((v, k) for k, v in cvn.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.015*\"don\" + 0.011*\"man\" + 0.009*\"day\" + 0.009*\"thing\" + 0.008*\"life\" + 0.008*\"cause\" + 0.007*\"way\" + 0.007*\"guy\" + 0.006*\"shit\" + 0.006*\"house\"'),\n",
       " (1,\n",
       "  '0.013*\"don\" + 0.009*\"thing\" + 0.008*\"shit\" + 0.008*\"day\" + 0.008*\"man\" + 0.007*\"way\" + 0.007*\"years\" + 0.006*\"cause\" + 0.006*\"life\" + 0.005*\"guy\"')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's start with 2 topics\n",
    "ldan = models.LdaModel(corpus=corpusn, num_topics=2, id2word=id2wordn, passes=10)\n",
    "ldan.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.015*\"don\" + 0.012*\"man\" + 0.010*\"day\" + 0.010*\"cause\" + 0.009*\"thing\" + 0.008*\"house\" + 0.007*\"way\" + 0.007*\"guy\" + 0.006*\"kids\" + 0.006*\"ass\"'),\n",
       " (1,\n",
       "  '0.013*\"don\" + 0.010*\"day\" + 0.007*\"thing\" + 0.007*\"life\" + 0.007*\"joke\" + 0.006*\"dad\" + 0.006*\"years\" + 0.006*\"school\" + 0.005*\"man\" + 0.005*\"year\"'),\n",
       " (2,\n",
       "  '0.013*\"don\" + 0.011*\"shit\" + 0.011*\"thing\" + 0.010*\"man\" + 0.010*\"life\" + 0.008*\"way\" + 0.008*\"guy\" + 0.008*\"cause\" + 0.006*\"everybody\" + 0.006*\"day\"')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try topics = 3\n",
    "ldan = models.LdaModel(corpus=corpusn, num_topics=3, id2word=id2wordn, passes=10)\n",
    "ldan.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.016*\"don\" + 0.012*\"man\" + 0.011*\"day\" + 0.010*\"cause\" + 0.009*\"thing\" + 0.008*\"house\" + 0.008*\"way\" + 0.008*\"guy\" + 0.006*\"kids\" + 0.006*\"ass\"'),\n",
       " (1,\n",
       "  '0.015*\"life\" + 0.013*\"don\" + 0.011*\"thing\" + 0.011*\"man\" + 0.010*\"cause\" + 0.010*\"kids\" + 0.009*\"shit\" + 0.008*\"way\" + 0.007*\"guy\" + 0.007*\"everybody\"'),\n",
       " (2,\n",
       "  '0.011*\"don\" + 0.010*\"man\" + 0.008*\"shit\" + 0.008*\"dad\" + 0.007*\"life\" + 0.007*\"fuck\" + 0.006*\"night\" + 0.006*\"didn\" + 0.005*\"way\" + 0.005*\"hey\"'),\n",
       " (3,\n",
       "  '0.013*\"don\" + 0.011*\"thing\" + 0.011*\"day\" + 0.008*\"shit\" + 0.008*\"years\" + 0.007*\"way\" + 0.006*\"joke\" + 0.006*\"year\" + 0.006*\"cause\" + 0.006*\"man\"')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try 4 topics\n",
    "ldan = models.LdaModel(corpus=corpusn, num_topics=4, id2word=id2wordn, passes=10)\n",
    "ldan.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling - Attempt #3 (Nouns and Adjectives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's create a function to pull out nouns from a string of text\n",
    "def nouns_adj(text):\n",
    "    '''Given a string of text, tokenize the text and pull out only the nouns and adjectives.'''\n",
    "    is_noun_adj = lambda pos: pos[:2] == 'NN' or pos[:2] == 'JJ'\n",
    "    tokenized = word_tokenize(text)\n",
    "    nouns_adj = [word for (word, pos) in pos_tag(tokenized) if is_noun_adj(pos)] \n",
    "    return ' '.join(nouns_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ali</th>\n",
       "      <td>ladies gentlemen stage ali wong hello welcome ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anthony</th>\n",
       "      <td>thank san francisco thank good people told tap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bill</th>\n",
       "      <td>right thank thank greater atlanta georgia area...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dave</th>\n",
       "      <td>dave dirty jokes living hard work signifies tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gabriel</th>\n",
       "      <td>state name martin moreno gabriel iglesias plus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hasan</th>\n",
       "      <td>home netflix special chicago new york nah son ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jim</th>\n",
       "      <td>ladies gentlemen stage jim jefferies thank swe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joe</th>\n",
       "      <td>ladies gentlemen joe san francisco thanks appr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>john</th>\n",
       "      <td>boyish charm sharp wit former snl writer john ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kevin</th>\n",
       "      <td>netflix house downstairs work champ homework e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>louis</th>\n",
       "      <td>intro music let roll lights thank much appreci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mike</th>\n",
       "      <td>wow hey thanks hey seattle nice see look crazy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ricky</th>\n",
       "      <td>hello great thank fuck thank welcome gon harde...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                transcript\n",
       "ali      ladies gentlemen stage ali wong hello welcome ...\n",
       "anthony  thank san francisco thank good people told tap...\n",
       "bill     right thank thank greater atlanta georgia area...\n",
       "dave     dave dirty jokes living hard work signifies tr...\n",
       "gabriel  state name martin moreno gabriel iglesias plus...\n",
       "hasan    home netflix special chicago new york nah son ...\n",
       "jim      ladies gentlemen stage jim jefferies thank swe...\n",
       "joe      ladies gentlemen joe san francisco thanks appr...\n",
       "john     boyish charm sharp wit former snl writer john ...\n",
       "kevin    netflix house downstairs work champ homework e...\n",
       "louis    intro music let roll lights thank much appreci...\n",
       "mike     wow hey thanks hey seattle nice see look crazy...\n",
       "ricky    hello great thank fuck thank welcome gon harde..."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the nouns function to the transcripts to filter only on nouns\n",
    "data_nouns_adj = pd.DataFrame(data_clean.transcript.apply(nouns_adj))\n",
    "data_nouns_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ladies</th>\n",
       "      <th>gentlemen</th>\n",
       "      <th>ali</th>\n",
       "      <th>wong</th>\n",
       "      <th>hello</th>\n",
       "      <th>welcome</th>\n",
       "      <th>exciting</th>\n",
       "      <th>older</th>\n",
       "      <th>automatic</th>\n",
       "      <th>jealous</th>\n",
       "      <th>...</th>\n",
       "      <th>tissue</th>\n",
       "      <th>madhouse</th>\n",
       "      <th>shook</th>\n",
       "      <th>beeline</th>\n",
       "      <th>adversity</th>\n",
       "      <th>bognor</th>\n",
       "      <th>boot</th>\n",
       "      <th>packet</th>\n",
       "      <th>copper</th>\n",
       "      <th>helmets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ali</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anthony</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bill</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dave</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gabriel</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hasan</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jim</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joe</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>john</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kevin</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>louis</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mike</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ricky</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13 rows × 5535 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ladies  gentlemen  ali  wong  hello  welcome  exciting  older  \\\n",
       "ali           0          0    0     1      0        0         0      2   \n",
       "anthony       0          0    0     0      0        0         0      0   \n",
       "bill          1          0    0     0      1        0         0      1   \n",
       "dave          0          0    0     0      0        0         0      0   \n",
       "gabriel       0          1    0     1      0        0         0      0   \n",
       "hasan         0          0    0     0      0        0         0      1   \n",
       "jim           0          0    0     0      0        0         0      3   \n",
       "joe           0          0    0     0      0        0         0      2   \n",
       "john          0          0    0     0      0        0         0      3   \n",
       "kevin         0          0    2     0      0        0         0      2   \n",
       "louis         0          0    1     0      0        0         0      1   \n",
       "mike          0          0    0     0      0        0         0      0   \n",
       "ricky         0          0    0     0      0        1         1      2   \n",
       "\n",
       "         automatic  jealous  ...  tissue  madhouse  shook  beeline  adversity  \\\n",
       "ali              0        0  ...       0         0      0        0          0   \n",
       "anthony          2        0  ...      10         0      0        0          0   \n",
       "bill             0        0  ...       0         0      0        0          1   \n",
       "dave             0        1  ...       0         0      0        0          0   \n",
       "gabriel          0        0  ...       0         0      0        0          0   \n",
       "hasan            0        0  ...       0         2      1        0          0   \n",
       "jim              0        0  ...       0         0      0        0          0   \n",
       "joe              0        0  ...       0         0      0        0          0   \n",
       "john             0        0  ...       0         0      0        0          0   \n",
       "kevin            0        0  ...       0         0      0        0          0   \n",
       "louis            0        0  ...       0         0      0        0          0   \n",
       "mike             0        0  ...       0         0      0        2          0   \n",
       "ricky            0        0  ...       0         0      0        0          0   \n",
       "\n",
       "         bognor  boot  packet  copper  helmets  \n",
       "ali           1     0       0       0        0  \n",
       "anthony       0     0       0       0        0  \n",
       "bill          1     1       0       0        0  \n",
       "dave          0     0       0       0        0  \n",
       "gabriel       0     0       1       0        4  \n",
       "hasan         0     0       0       0        0  \n",
       "jim           0     0       0       0        0  \n",
       "joe           0     0       0       0        0  \n",
       "john          0     0       0       0        0  \n",
       "kevin         0     1       0       0        0  \n",
       "louis         0     0       0       0        0  \n",
       "mike          0     0       0       0        0  \n",
       "ricky         0     0       0       1        0  \n",
       "\n",
       "[13 rows x 5535 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new document-term matrix using only nouns and adjectives, also remove common words with max_df\n",
    "cvna = CountVectorizer(stop_words=a_stop_words, max_df=.8)\n",
    "data_cvna = cvna.fit_transform(data_nouns_adj.transcript)\n",
    "data_dtmna = pd.DataFrame(data_cvna.toarray(), columns=cvna.vocabulary_)\n",
    "data_dtmna.index = data_nouns_adj.index\n",
    "data_dtmna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the gensim corpus\n",
    "corpusna = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(data_dtmna.transpose()))\n",
    "\n",
    "# Create the vocabulary dictionary\n",
    "id2wordna = dict((v, k) for k, v in cvna.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.004*\"fuck\" + 0.004*\"jenny\" + 0.004*\"mom\" + 0.003*\"parents\" + 0.003*\"wife\" + 0.003*\"clinton\" + 0.003*\"friend\" + 0.003*\"men\" + 0.002*\"young\" + 0.002*\"date\"'),\n",
       " (1,\n",
       "  '0.005*\"fuck\" + 0.005*\"joke\" + 0.004*\"dude\" + 0.004*\"kid\" + 0.004*\"mom\" + 0.003*\"fucking\" + 0.003*\"jokes\" + 0.003*\"wife\" + 0.003*\"door\" + 0.003*\"dick\"')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's start with 2 topics\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=2, id2word=id2wordna, passes=10)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.004*\"mom\" + 0.004*\"joke\" + 0.004*\"dogs\" + 0.003*\"hasan\" + 0.003*\"fuck\" + 0.003*\"parents\" + 0.003*\"fact\" + 0.003*\"kid\" + 0.003*\"country\" + 0.003*\"martin\"'),\n",
       " (1,\n",
       "  '0.007*\"fuck\" + 0.005*\"men\" + 0.005*\"wife\" + 0.004*\"joke\" + 0.004*\"dick\" + 0.004*\"kid\" + 0.003*\"guns\" + 0.003*\"dude\" + 0.003*\"mad\" + 0.003*\"jokes\"'),\n",
       " (2,\n",
       "  '0.005*\"dude\" + 0.005*\"jenny\" + 0.005*\"mom\" + 0.004*\"fuck\" + 0.004*\"wife\" + 0.004*\"clinton\" + 0.004*\"parents\" + 0.003*\"half\" + 0.003*\"john\" + 0.003*\"cow\"')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try 3 topics\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=3, id2word=id2wordna, passes=10)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.009*\"fuck\" + 0.006*\"dude\" + 0.005*\"wife\" + 0.005*\"men\" + 0.004*\"kid\" + 0.004*\"joke\" + 0.004*\"dick\" + 0.004*\"fucking\" + 0.003*\"gun\" + 0.003*\"guns\"'),\n",
       " (1,\n",
       "  '0.005*\"jenny\" + 0.005*\"mom\" + 0.004*\"parents\" + 0.004*\"hasan\" + 0.004*\"door\" + 0.004*\"martin\" + 0.003*\"frankie\" + 0.003*\"country\" + 0.003*\"dogs\" + 0.003*\"friend\"'),\n",
       " (2,\n",
       "  '0.005*\"mom\" + 0.004*\"clinton\" + 0.004*\"joke\" + 0.004*\"dead\" + 0.003*\"fuck\" + 0.003*\"parents\" + 0.003*\"food\" + 0.003*\"nuts\" + 0.003*\"cow\" + 0.003*\"half\"'),\n",
       " (3,\n",
       "  '0.000*\"mom\" + 0.000*\"fuck\" + 0.000*\"friend\" + 0.000*\"wife\" + 0.000*\"dude\" + 0.000*\"joke\" + 0.000*\"fucking\" + 0.000*\"kid\" + 0.000*\"door\" + 0.000*\"dog\"')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try 4 topics\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=4, id2word=id2wordna, passes=10)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify Topics in Each Document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of the 9 topic models we looked at, the nouns and adjectives, 4 topic one made the most sense. So let's pull that down here and run it through some more iterations to get more fine-tuned topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.006*\"jenny\" + 0.005*\"wife\" + 0.005*\"dude\" + 0.005*\"fuck\" + 0.004*\"vid\" + 0.004*\"ckin\" + 0.003*\"somebody\" + 0.003*\"dick\" + 0.003*\"cking\" + 0.003*\"kid\"'),\n",
       " (1,\n",
       "  '0.006*\"mom\" + 0.005*\"fuck\" + 0.004*\"joke\" + 0.004*\"men\" + 0.004*\"guns\" + 0.004*\"kid\" + 0.004*\"clinton\" + 0.004*\"wife\" + 0.003*\"dick\" + 0.003*\"anthony\"'),\n",
       " (2,\n",
       "  '0.005*\"joke\" + 0.005*\"fuck\" + 0.005*\"dogs\" + 0.004*\"martin\" + 0.004*\"fact\" + 0.003*\"frankie\" + 0.003*\"tweet\" + 0.003*\"long\" + 0.003*\"dead\" + 0.003*\"mexican\"'),\n",
       " (3,\n",
       "  '0.006*\"mom\" + 0.005*\"hasan\" + 0.005*\"fuck\" + 0.005*\"kid\" + 0.005*\"parents\" + 0.004*\"half\" + 0.004*\"brown\" + 0.004*\"date\" + 0.004*\"dude\" + 0.003*\"york\"')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Our final LDA model (for now)\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=4, id2word=id2wordna, passes=80)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These four topics look pretty decent. Let's settle on these for now.\n",
    "* Topic 0: profanity\n",
    "* Topic 1: guns\n",
    "* Topic 2: dogs,tweet\n",
    "* Topic 3: mom, parents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3, 'ali'),\n",
       " (1, 'anthony'),\n",
       " (0, 'bill'),\n",
       " (2, 'dave'),\n",
       " (2, 'gabriel'),\n",
       " (3, 'hasan'),\n",
       " (1, 'jim'),\n",
       " (1, 'joe'),\n",
       " (1, 'john'),\n",
       " (0, 'kevin'),\n",
       " (3, 'louis'),\n",
       " (0, 'mike'),\n",
       " (2, 'ricky')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's take a look at which topics each transcript contains\n",
    "corpus_transformed = ldana[corpusna]\n",
    "list(zip([a for [(a,b)] in corpus_transformed], data_dtmna.index))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "For a first pass of LDA, these kind of make sense to me, so we'll call it a day for now.\n",
    "* Topic 0: profanity[Bill,Kevin,Mike]\n",
    "* Topic 1: guns[Anthony,Jim,Joe,John]\n",
    "* Topic 2: dogs,tweet[Gabriel,Dave,Ricky]\n",
    "* Topic 3: mom, parents[Ali,Louis,Hasan]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment:\n",
    "1. Try further modifying the parameters of the topic models above and see if you can get better topics.\n",
    "2. Create a new topic model that includes terms from a different [part of speech](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html) and see if you can get better topics."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment Solutions\n",
    "1. Try further modifying the parameters of the topic models above and see if you can get better topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda_model = models.LdaModel(corpus=corpusna, id2word=id2wordna, num_topics=5, passes=100, alpha=0.5, eta=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.008*\"wife\" + 0.006*\"mom\" + 0.005*\"dick\" + 0.005*\"joke\" + 0.005*\"fuck\" + 0.004*\"mad\" + 0.004*\"men\" + 0.004*\"jokes\" + 0.004*\"kid\" + 0.004*\"clinton\"'),\n",
       " (1,\n",
       "  '0.011*\"fuck\" + 0.008*\"joke\" + 0.008*\"fucking\" + 0.008*\"guns\" + 0.006*\"cunt\" + 0.006*\"class\" + 0.005*\"nuts\" + 0.005*\"jenner\" + 0.005*\"men\" + 0.005*\"hampstead\"'),\n",
       " (2,\n",
       "  '0.017*\"fuck\" + 0.016*\"dude\" + 0.009*\"gun\" + 0.009*\"fucking\" + 0.008*\"jesus\" + 0.007*\"kid\" + 0.006*\"sense\" + 0.006*\"helicopter\" + 0.006*\"religion\" + 0.006*\"trouble\"'),\n",
       " (3,\n",
       "  '0.009*\"jenny\" + 0.006*\"martin\" + 0.006*\"frankie\" + 0.006*\"dogs\" + 0.005*\"fact\" + 0.005*\"friend\" + 0.005*\"mexican\" + 0.005*\"canelo\" + 0.005*\"candy\" + 0.005*\"weeks\"'),\n",
       " (4,\n",
       "  '0.008*\"mom\" + 0.007*\"hasan\" + 0.006*\"fuck\" + 0.006*\"kid\" + 0.006*\"parents\" + 0.005*\"brown\" + 0.005*\"half\" + 0.005*\"date\" + 0.005*\"dude\" + 0.004*\"york\"')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model.print_topics()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These four topics look pretty decent. Let's settle on these for now.\n",
    "* Topic 0: wife,mom\n",
    "* Topic 1: profanity\n",
    "* Topic 2: gun,kid,religion\n",
    "* Topic 3: dogs,mexican\n",
    "* Topic 4: parents,brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4, 'ali'),\n",
       " (0, 'anthony'),\n",
       " (2, 'bill'),\n",
       " (0, 'dave'),\n",
       " (3, 'gabriel'),\n",
       " (4, 'hasan'),\n",
       " (1, 'jim'),\n",
       " (0, 'joe'),\n",
       " (0, 'john'),\n",
       " (0, 'kevin'),\n",
       " (4, 'louis'),\n",
       " (3, 'mike'),\n",
       " (1, 'ricky')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_transformed = lda_model[corpusna]\n",
    "list(zip([a for [(a,b)] in corpus_transformed], data_dtmna.index))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Topic 0: wife,mom [Anthony,Dave,Joe,John,Kevin]\n",
    "* Topic 1: profanity [Jim,Ricky]\n",
    "* Topic 2: gun,kid,religion [Bill]\n",
    "* Topic 3: dogs,mexican [Gabriel,Mike]\n",
    "* Topic 4: parents,brown [Ali,Hasan,Louis]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Create a new topic model that includes terms from a different [part of speech](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html) and see if you can get better topics."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* * ##### Topic Modeling - Attempt #4 (Nouns,Adjectives,Verbs and no proper nouns) \n",
    "* ##### [reference](https://sites.google.com/site/partofspeechhelp/home)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a function to pull out nouns from a string of text\n",
    "def nouns_adj_verb(text):\n",
    "    '''Given a string of text, tokenize the text and pull out only the nouns and adjectives.'''\n",
    "    is_noun_adj_verb = lambda pos: (pos[:2] == 'NN' or pos[:2] == 'JJ' or pos[:2] == 'VB') and pos[:2] != 'NNP'\n",
    "    tokenized = word_tokenize(text)\n",
    "    nouns_adj_verb = [word for (word, pos) in pos_tag(tokenized) if is_noun_adj_verb(pos)] \n",
    "    return ' '.join(nouns_adj_verb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ali</th>\n",
       "      <td>ladies gentlemen please welcome stage ali wong...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anthony</th>\n",
       "      <td>thank thank thank san francisco thank good peo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bill</th>\n",
       "      <td>right thank thank thank thank are going thank ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dave</th>\n",
       "      <td>dave tells dirty jokes living stare hard work ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gabriel</th>\n",
       "      <td>please state name martin moreno know been tour...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hasan</th>\n",
       "      <td>davis home had bring netflix said want special...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jim</th>\n",
       "      <td>ladies gentlemen please welcome stage jim jeff...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joe</th>\n",
       "      <td>ladies gentlemen welcome joe rogan going san f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>john</th>\n",
       "      <td>armed boyish charm sharp wit former snl writer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kevin</th>\n",
       "      <td>streaming netflix was looking house chilling d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>louis</th>\n",
       "      <td>intro fade music let roll hold lights lights t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mike</th>\n",
       "      <td>wow hey thank thanks thank guys hey seattle ni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ricky</th>\n",
       "      <td>hello hello doing great thank wow calm shut fu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                transcript\n",
       "ali      ladies gentlemen please welcome stage ali wong...\n",
       "anthony  thank thank thank san francisco thank good peo...\n",
       "bill     right thank thank thank thank are going thank ...\n",
       "dave     dave tells dirty jokes living stare hard work ...\n",
       "gabriel  please state name martin moreno know been tour...\n",
       "hasan    davis home had bring netflix said want special...\n",
       "jim      ladies gentlemen please welcome stage jim jeff...\n",
       "joe      ladies gentlemen welcome joe rogan going san f...\n",
       "john     armed boyish charm sharp wit former snl writer...\n",
       "kevin    streaming netflix was looking house chilling d...\n",
       "louis    intro fade music let roll hold lights lights t...\n",
       "mike     wow hey thank thanks thank guys hey seattle ni...\n",
       "ricky    hello hello doing great thank wow calm shut fu..."
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_nouns_adj_verb = pd.DataFrame(data_clean.transcript.apply(nouns_adj_verb))\n",
    "data_nouns_adj_verb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ladies</th>\n",
       "      <th>gentlemen</th>\n",
       "      <th>welcome</th>\n",
       "      <th>ali</th>\n",
       "      <th>wong</th>\n",
       "      <th>hello</th>\n",
       "      <th>pee</th>\n",
       "      <th>exciting</th>\n",
       "      <th>turned</th>\n",
       "      <th>appreciate</th>\n",
       "      <th>...</th>\n",
       "      <th>beeline</th>\n",
       "      <th>adversity</th>\n",
       "      <th>bullet</th>\n",
       "      <th>bognor</th>\n",
       "      <th>boot</th>\n",
       "      <th>packet</th>\n",
       "      <th>copper</th>\n",
       "      <th>wondered</th>\n",
       "      <th>helmets</th>\n",
       "      <th>prolefeed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ali</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anthony</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bill</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dave</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gabriel</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hasan</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jim</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joe</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>john</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kevin</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>louis</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mike</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ricky</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13 rows × 6943 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ladies  gentlemen  welcome  ali  wong  hello  pee  exciting  turned  \\\n",
       "ali           0          0        0    1     0      0    0         2       0   \n",
       "anthony       0          0        0    0     0      0    0         0       2   \n",
       "bill          1          0        0    0     1      0    0         1       0   \n",
       "dave          0          1        0    0     0      0    0         0       0   \n",
       "gabriel       0          1        0    1     0      0    0         0       0   \n",
       "hasan         0          0        0    0     0      0    0         1       0   \n",
       "jim           0          0        0    0     0      0    0         3       0   \n",
       "joe           0          0        0    0     0      0    0         2       0   \n",
       "john          0          0        0    0     0      0    0         3       0   \n",
       "kevin         0          0        3    0     0      0    0         2       0   \n",
       "louis         0          0        2    0     0      0    0         1       0   \n",
       "mike          0          0        0    0     0      0    0         0       0   \n",
       "ricky         0          0        0    0     0      1    1         2       0   \n",
       "\n",
       "         appreciate  ...  beeline  adversity  bullet  bognor  boot  packet  \\\n",
       "ali               0  ...        0          0       0       0     1       0   \n",
       "anthony           0  ...        0          0       0       0     0       0   \n",
       "bill              0  ...        0          0       0       1     1       1   \n",
       "dave              1  ...        0          0       0       0     0       0   \n",
       "gabriel           0  ...        0          0       0       0     0       0   \n",
       "hasan             0  ...        1          0       0       0     0       0   \n",
       "jim               0  ...        0          0       0       0     0       0   \n",
       "joe               0  ...        0          0       0       0     0       0   \n",
       "john              0  ...        0          0       0       0     0       0   \n",
       "kevin             0  ...        0          0       0       0     0       1   \n",
       "louis             0  ...        0          0       1       0     0       0   \n",
       "mike              0  ...        0          2       0       0     0       0   \n",
       "ricky             0  ...        0          0       0       0     0       0   \n",
       "\n",
       "         copper  wondered  helmets  prolefeed  \n",
       "ali           0         0        0          0  \n",
       "anthony       0         0        0          0  \n",
       "bill          0         1        0          0  \n",
       "dave          0         0        0          0  \n",
       "gabriel       1         0        0          4  \n",
       "hasan         0         0        0          0  \n",
       "jim           0         0        0          0  \n",
       "joe           0         0        0          0  \n",
       "john          0         0        0          0  \n",
       "kevin         0         0        0          0  \n",
       "louis         0         0        0          0  \n",
       "mike          0         0        0          0  \n",
       "ricky         0         0        1          0  \n",
       "\n",
       "[13 rows x 6943 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvnav = CountVectorizer(stop_words=a_stop_words, max_df=.8)\n",
    "data_cvnav = cvnav.fit_transform(data_nouns_adj_verb.transcript)\n",
    "data_dtmnav = pd.DataFrame(data_cvnav.toarray(), columns=cvnav.vocabulary_)\n",
    "data_dtmnav.index = data_nouns_adj_verb.index\n",
    "data_dtmnav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpusnav = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(data_dtmnav.transpose()))\n",
    "\n",
    "# Create the vocabulary dictionary\n",
    "id2wordnav = dict((v, k) for k, v in cvnav.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_nav = models.LdaModel(corpus=corpusnav, id2word=id2wordnav, num_topics=5, passes=100, alpha=0.5, eta=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.007*\"jenny\" + 0.005*\"martin\" + 0.005*\"friend\" + 0.004*\"frankie\" + 0.004*\"dogs\" + 0.004*\"canelo\" + 0.004*\"fact\" + 0.004*\"couldn\" + 0.004*\"mexican\" + 0.004*\"candy\"'),\n",
       " (1,\n",
       "  '0.013*\"murder\" + 0.013*\"tit\" + 0.010*\"fuck\" + 0.009*\"date\" + 0.009*\"dog\" + 0.009*\"tits\" + 0.008*\"worst\" + 0.008*\"older\" + 0.008*\"food\" + 0.007*\"kid\"'),\n",
       " (2,\n",
       "  '0.021*\"fucking\" + 0.011*\"fuck\" + 0.008*\"joke\" + 0.006*\"kid\" + 0.004*\"gun\" + 0.004*\"wan\" + 0.003*\"guns\" + 0.003*\"dead\" + 0.003*\"hate\" + 0.003*\"sleep\"'),\n",
       " (3,\n",
       "  '0.011*\"fuck\" + 0.010*\"fucking\" + 0.008*\"hasan\" + 0.006*\"mom\" + 0.005*\"brown\" + 0.005*\"parents\" + 0.005*\"gay\" + 0.004*\"kevin\" + 0.004*\"door\" + 0.004*\"called\"'),\n",
       " (4,\n",
       "  '0.026*\"cking\" + 0.007*\"wife\" + 0.006*\"dick\" + 0.006*\"clinton\" + 0.006*\"wan\" + 0.006*\"kid\" + 0.005*\"mom\" + 0.005*\"ain\" + 0.004*\"tch\" + 0.004*\"ckin\"')]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_nav.print_topics()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Topic 0: dogs,friend,mexican \n",
    "* Topic 1: profanity \n",
    "* Topic 2: gun,kid \n",
    "* Topic 3: mom,parents,gay \n",
    "* Topic 4: wife,kid,mom "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 'ali'),\n",
       " (2, 'anthony'),\n",
       " (2, 'bill'),\n",
       " (3, 'dave'),\n",
       " (0, 'gabriel'),\n",
       " (3, 'hasan'),\n",
       " (2, 'jim'),\n",
       " (4, 'joe'),\n",
       " (4, 'john'),\n",
       " (4, 'kevin'),\n",
       " (1, 'louis'),\n",
       " (0, 'mike'),\n",
       " (2, 'ricky')]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_transformed = lda_nav[corpusnav]\n",
    "list(zip([a for [(a,b)] in corpus_transformed], data_dtmnav.index))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Topic 0: dogs,friend,mexican [Gabriel,Mike]\n",
    "* Topic 1: profanity [Louis]\n",
    "* Topic 2: gun,kid [Ali,Anthony,Bill,Jim,Ricky]\n",
    "* Topic 3: mom,parents,gay [Dave,Hasan]\n",
    "* Topic 4: wife,kid,mom [Joe,John,Kevin]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### * [reference](https://towardsdatascience.com/latent-dirichlet-allocation-lda-a-guide-to-probabilistic-modeling-approach-for-topic-discovery-8cb97c08da3c)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "7758e92e9a61d7a3490898707f7eeb937c85e9d1e8d4e877cc6c187218f226d5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
